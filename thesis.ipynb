{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1y4qqH-Xv_sst8YgW6yRuL7oG8S_HfXx_",
      "authorship_tag": "ABX9TyOoH+s4o4y0uRPHO6HxPtPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lailaabukhalaf/lailaabukhalaf/blob/main/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "why we use torch ?\n",
        "\n",
        "* Building and training neural networks\n",
        "\n",
        "* Handling tensors (multi-dimensional arrays like NumPy, but faster and GPU-ready)\n",
        "\n",
        "* Performing automatic differentiation (for backpropagation)\n",
        "\n",
        "**torchvision is a companion package for image tasks**\n",
        "\n",
        "It includes:\n",
        "\n",
        "Popular datasets (MNIST, CIFAR-10, ImageNet)\n",
        "\n",
        "Image transforms (resize, normalize, augment)\n",
        "\n",
        "Pre-trained models (ResNet, VGG, etc.)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FKtNFQBwe_ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "4gs2R3JFgBHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform function**\n",
        "\n",
        "the transform is a preprocessing function (or a chain of functions) that you apply automatically to every image when it's loaded.\n",
        "\n",
        "every time you load an image, do this to it before giving it to my model.\n",
        "\n",
        "\n",
        "What kind of things can transform do?\n",
        "\n",
        "* Convert images to tensors (ToTensor())\n",
        "\n",
        "* Normalize pixel values (Normalize(mean, std))\n",
        "\n",
        "* Resize, crop, or flip images\n",
        "\n",
        "* Augment images (e.g., rotate, change brightness, zoom)\n",
        "\n",
        "* Chain multiple steps together with transforms.Compose([...])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zwjoXpAogGN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define transformations (convert to tensor and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0,1] tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std of MNIST\n",
        "])\n"
      ],
      "metadata": {
        "id": "mfGZqgDSzAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "what does tranformer do ?  example from raw to after transformation ( convert them to tensors and also normalize)"
      ],
      "metadata": {
        "id": "yETsvi_Rj7rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define transformations\n",
        "to_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "#Load MNIST dataset without any transform\n",
        "raw_dataset = datasets.MNIST(root=\"./data\", train=True, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ7OWRVojiC0",
        "outputId": "723b8d5d-bc4f-48c1-e60b-5ad5ae1e2ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.69MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.99MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = raw_dataset[0]  # Get the first image\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "6T_g54IijlxA",
        "outputId": "14993595-4ca0-4169-c2f7-b046c221dd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==\n"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ToTensor and Normalize separately\n",
        "image_tensor = to_tensor(image)\n",
        "image_normalized = normalize(image_tensor)"
      ],
      "metadata": {
        "id": "jUb4IJbcjxbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all three versions\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Original image\n",
        "axs[0].imshow(image, cmap=\"gray\")\n",
        "axs[0].set_title(\"Original\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "# After ToTensor (still looks normal)\n",
        "axs[1].imshow(image_tensor.squeeze(), cmap=\"gray\")\n",
        "axs[1].set_title(\"After ToTensor()\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "# After Normalize (looks weird but it's expected)\n",
        "axs[2].imshow(image_normalized.squeeze(), cmap=\"gray\")\n",
        "axs[2].set_title(\"After Normalize()\")\n",
        "axs[2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rgC4UjHfjeKu",
        "outputId": "c1de84bc-448a-4600-afa6-031e5b8b8aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "why do that ?\n",
        "\n",
        "Because raw images (like PNG or JPEG files) can't be directly used in neural networks. We need to:\n",
        "\n",
        "Convert them to tensors\n",
        "\n",
        "Scale or normalize values\n",
        "\n",
        "Make sure they’re the same size and format to fit it in the model"
      ],
      "metadata": {
        "id": "QQkzQQ3GiFDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any model we need to have\n",
        "\n",
        "* Training Data\n",
        "* Testing Data\n",
        "\n",
        "train = True meaning this data is for training\n",
        "\n",
        "train = false.  - for testing\n",
        "\n",
        "\n",
        "then we need to apply the transformation on the dataset\n"
      ],
      "metadata": {
        "id": "rAvg8GVld0Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training and test datasets\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "iYg8ZyNAi9y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Be8FacHfiIZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle=True, it means the data will be randomly shuffled at the beginning of each training epoch.\n",
        "\n",
        "Why shuffle?\n",
        "\n",
        "Prevents the model from learning the order of the data.\n",
        "\n",
        "Improves generalization.\n",
        "\n",
        "Especially useful when your data might have some order (e.g., grouped by class).\n",
        "\n",
        "\n",
        "- shuffle=True → randomly rearrange the training data every epoch.\n",
        "\n",
        "- shuffle=False → keep the order as is (used for test/validation)."
      ],
      "metadata": {
        "id": "uNJUsAtYejtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size=64: The data will be split into mini-batches of 64 samples. This helps improve training speed and generalization.\n",
        "\n",
        "Efficient memory usage: Data is loaded in chunks (batches) instead of all at once.\n",
        "\n",
        "Easy iteration: we can loop through train_loader or test_loader using a for loop."
      ],
      "metadata": {
        "id": "aRkP-MansfrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "FjPb9sAmezOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grayscale \t    1 channel\n",
        "\n",
        "RGB            \t3 channels (Red, Green, Blue)\n",
        "\n",
        "Conv2d(1, 32, kernel_size=3)\n",
        "\n",
        "Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "filter size 3*3 go through each input\n",
        "Input: 32 channels (from previous layer)\n",
        "\n",
        "Output: 64 channels (64 learned filters)\n",
        "\n"
      ],
      "metadata": {
        "id": "NRKVZJ1E7TGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "why its 8 16 32 64\n",
        "\n",
        "\n",
        "GPUs and CPUs are optimized for powers of 2 because of how memory is stored and accessed.\n",
        "\n",
        "Powers of 2 align well with hardware (e.g., 8-bit, 16-bit, 32-bit memory blocks), which makes matrix operations faster.\n",
        "\n",
        "As you go deeper in the network, each layer captures more abstract features. 32-> 64-> 128,...\n",
        "\n",
        "\n",
        "So you increase the number of filters to capture more complexity."
      ],
      "metadata": {
        "id": "ky7UGLVrBQ9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we Use 7 or 10?\n",
        "Yes, you technically can. PyTorch lets you do this:\n",
        "\n",
        "Conv2d(1, 7, kernel_size=3)\n",
        "\n",
        "It will work! But:\n",
        "\n",
        "You’ll lose some hardware speed benefits.\n",
        "\n",
        "It's uncommon, so harder to compare your architecture with others.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rl1bYk5VBoGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import sys\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "4KgsEcFWse3v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn is used for layers (like Conv2d, Linear).\n",
        "\n",
        "F gives access to functional APIs (like F.relu, F.log_softmax)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e7xS9Qj9MT2M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsiqLkw8VnXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcyZ5jmOd64T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}